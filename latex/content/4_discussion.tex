\section{Discussion}
Kernel PCA proposes an efficient way to compute principal components on a high-dimensional feature space that is related to the input space in a non-linear way. It does that by using a kernel to represent the dot product between two vectors in the feature space. This means that it can better describe non-linearities in the data.
In our experiments non-linear Kernel PCA gave better results than standard linear PCA when extracting features and classifying using a linear classifier. This is advantage that could replicate from the original article. 
Another advantage is that we only need to solve an eigenvalue problem, as we do in standard PCA, and not use any non-linear optimization method. This is an advantage over other forms of non-linear PCA \cite{scholkopf1997kernel}.

The paper introduces several different kernels that can be used in the method and have previously been successfully used in Support Vector Machines. This knowledge can be used in kernel PCA but also in any other algorithm that only requires the inner product between vectors. However, the article only describes results from experiments for a polynomial kernel function. It would be interesting to reproduce the experiment using a different kernel function, and see how well this algorithm performs. In general use a kernel that is specific to the problem in order obtain a better performance.

The kernel PCA is not suitable for large datasets as PCA is not, since the method is computationally expensive and we cannot fit our data using conventional data structures in RAM and an out-of-core computation is required \cite{halko2011algorithm}. It would be interesting to adapt current PCA algorithms that attempt to take care of these computational problem to a kernel PCA method.

Eventually, we have noticed that when we computed the covariance matrix, $\bar{C}$, in derivation of the method, then the scaling factor applied was $l$. Since the use of this factor would induce a bias \cite{farebrother1999fitting}, we propose to improve the Kernel PCA method by using the Bessel's correction and therefore using $l - 1$ as a scaling factor to remove the bias. This is relevant because the method exclusively rely on the variance. This would change the eigenvalue problem slightly and thereby the normalization of the eigenvectors.
